{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Machine Learning Pipeline\n",
    "In this file there is the pipeline for the machine learning part of the project. The pipeline is composed by the following steps:\n",
    "1. Load the dataset\n",
    "2. Split the dataset into train, validation and test set\n",
    "3. Create the dataset class\n",
    "4. Create the model class\n",
    "5. Create the training loop\n",
    "6. Create the test loop\n",
    "\n",
    "For the hp search we use Weights and Biases."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:01.044310200Z",
     "start_time": "2023-05-20T13:20:56.416789100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mleonardo-berti07\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\leona/.netrc\n"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "# Log in to your W&B account\n",
    "wandb.login(key='d29d51017f4231b5149d36ad242526b374c9c60a')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Controlling the setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.0.1'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:01.051003300Z",
     "start_time": "2023-05-20T13:21:01.046337200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "NVIDIA GeForce RTX 3090\n",
      "Sat May 20 15:21:01 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.61                 Driver Version: 531.61       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090       WDDM | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   38C    P8               23W / 350W|    511MiB / 24576MiB |      5%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      7416    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      7956    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      7964    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8540    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9336    C+G   ...\\PyCharm 2023.1.1\\bin\\pycharm64.exe    N/A      |\n",
      "|    0   N/A  N/A     11036    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     12308    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     14104    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A     16092    C+G   ...on\\113.0.1774.42\\msedgewebview2.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.cudnn.enabled)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "\n",
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:01.141260300Z",
     "start_time": "2023-05-20T13:21:01.052059Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "#to debug\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:01.144336800Z",
     "start_time": "2023-05-20T13:21:01.143832300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "df = pd.read_csv(\"dataset_normalized.csv\")\n",
    "total_rows = df.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:01.379060300Z",
     "start_time": "2023-05-20T13:21:01.144336800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#splitting the dataset into train, validation and test set (70%, 10%, 20%)\n",
    "train_end = int(total_rows*0.7)\n",
    "val_end = int(total_rows*0.8)\n",
    "\n",
    "labels = df[\"market_value\"].values\n",
    "df = df.drop(columns=[\"market_value\"])\n",
    "\n",
    "to_encode = df[['citizenship', 'current_club_id', 'position', 'sub_position', \"competitions_id\", \"clubs_id\"]]\n",
    "#to_encode = df[['citizenship', 'current_club_id', 'position', 'sub_position']]\n",
    "df = df.drop(['citizenship', 'current_club_id', 'position', 'sub_position', \"competitions_id\", \"clubs_id\"], axis=1)\n",
    "\n",
    "train_to_encode = to_encode.iloc[:train_end].values\n",
    "val_to_encode = to_encode.iloc[train_end:val_end].values\n",
    "test_to_encode = to_encode.iloc[val_end:].values\n",
    "\n",
    "train_set = df.iloc[:train_end].values\n",
    "val_set = df.iloc[train_end:val_end].values\n",
    "test_set = df.iloc[val_end:].values\n",
    "\n",
    "y_train = labels[:train_end]\n",
    "y_val = labels[train_end:val_end]\n",
    "y_test = labels[val_end:]\n",
    "\n",
    "train_set_len = train_set.shape[0]\n",
    "val_set_len = val_set.shape[0]\n",
    "test_set_len = test_set.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:01.394576900Z",
     "start_time": "2023-05-20T13:21:01.382136600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the len of train set is: 95114\n",
      "the len of validation set is: 13588\n",
      "the len of test set is: 27176\n"
     ]
    }
   ],
   "source": [
    "print(\"the len of train set is: {}\".format(train_set_len))\n",
    "print(\"the len of validation set is: {}\".format(val_set_len))\n",
    "print(\"the len of test set is: {}\".format(test_set_len))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:01.400344300Z",
     "start_time": "2023-05-20T13:21:01.394576900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the pytorch Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, x, to_encode, y, length):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "\n",
    "        self.length = length\n",
    "        self.to_encode = torch.tensor(to_encode, device=device, dtype=torch.int32)\n",
    "        self.y = torch.tensor(y, device=device, dtype=torch.float32)\n",
    "        self.x = torch.tensor(x, device=device, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.to_encode[i] ,self.y[i]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:01.406775500Z",
     "start_time": "2023-05-20T13:21:01.400344300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "dataset_val = Dataset(val_set, val_to_encode, y_val, val_set_len)\n",
    "dataset_test = Dataset(test_set, test_to_encode, y_test, test_set_len)\n",
    "dataset_train = Dataset(train_set, train_to_encode, y_train, train_set_len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:02.464508500Z",
     "start_time": "2023-05-20T13:21:01.406775500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size of numerical features is: 18\n",
      "voc size citizenship is: 162\n",
      "voc size current club id is: 391\n",
      "voc size position is: 4\n",
      "voc size sub position is: 16\n",
      "voc size competitions is: 2727\n",
      "voc size clubs is: 8119\n",
      "the input size is: 44\n"
     ]
    }
   ],
   "source": [
    "#input parameters\n",
    "input_size = train_set.shape[1]\n",
    "voc_size_citizenship = len(to_encode['citizenship'].unique())\n",
    "voc_size_current_club_id = len(to_encode['current_club_id'].unique())\n",
    "voc_size_position = len(to_encode['position'].unique())\n",
    "voc_size_sub_position = len(to_encode['sub_position'].unique())\n",
    "voc_size_competitions = len(to_encode['competitions_id'].unique())\n",
    "voc_size_clubs = len(to_encode['clubs_id'].unique())\n",
    "#print them all\n",
    "print(\"input size of numerical features is: {}\".format(input_size))\n",
    "print(\"voc size citizenship is: {}\".format(voc_size_citizenship))\n",
    "print(\"voc size current club id is: {}\".format(voc_size_current_club_id))\n",
    "print(\"voc size position is: {}\".format(voc_size_position))\n",
    "print(\"voc size sub position is: {}\".format(voc_size_sub_position))\n",
    "print(\"voc size competitions is: {}\".format(voc_size_competitions))\n",
    "print(\"voc size clubs is: {}\".format(voc_size_clubs))\n",
    "\n",
    "emb_size_citizenship = int(voc_size_citizenship ** (1/4))\n",
    "emb_size_current_club_id = int(voc_size_current_club_id ** (1/4))\n",
    "emb_size_position = int(voc_size_position ** (1/4))\n",
    "emb_size_sub_position = int(voc_size_sub_position ** (1/4))\n",
    "emb_size_competitions = 0\n",
    "emb_size_clubs = 0\n",
    "emb_size_competitions = int(voc_size_competitions ** (1/4))\n",
    "emb_size_clubs = int(voc_size_clubs ** (1/4))\n",
    "\n",
    "total_input_size = input_size + emb_size_citizenship + emb_size_current_club_id + emb_size_position + emb_size_sub_position + emb_size_competitions + emb_size_clubs\n",
    "print(\"the input size is: \" + str(total_input_size))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:02.476167900Z",
     "start_time": "2023-05-20T13:21:02.464508500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Define a linear regression model class that inherits from nn.Module\n",
    "class LinearRegression(nn.Module):\n",
    "  # Define the constructor method that takes the input size and the vocabulary sizes of four categorical features as arguments\n",
    "  def __init__(self, input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, device):\n",
    "    # Call the parent constructor\n",
    "    super(LinearRegression, self).__init__()\n",
    "\n",
    "    # Compute the embedding sizes for each categorical feature using the fourth root of the vocabulary size\n",
    "    emb_size_citizenship = int(voc_size_citizenship ** (1/4))\n",
    "    emb_size_current_club_id = int(voc_size_current_club_id ** (1/4))\n",
    "    emb_size_position = int(voc_size_position ** (1/4))\n",
    "    emb_size_sub_position = int(voc_size_sub_position ** (1/4))\n",
    "    emb_size_competitions = int(voc_size_competitions ** (1/4))\n",
    "    emb_size_clubs = int(voc_size_clubs ** (1/4))\n",
    "\n",
    "    # Define embedding layers for each categorical feature using the computed embedding sizes\n",
    "    self.emb_cit = nn.Embedding(voc_size_citizenship, emb_size_citizenship, device=device)\n",
    "    self.emb_club = nn.Embedding(voc_size_current_club_id, emb_size_current_club_id, device=device)\n",
    "    self.emb_pos = nn.Embedding(voc_size_position, emb_size_position,  device=device)\n",
    "    self.emb_sub_pos = nn.Embedding(voc_size_sub_position, emb_size_sub_position, device=device)\n",
    "    self.emb_comp = nn.Embedding(voc_size_competitions, emb_size_competitions, device=device)\n",
    "    self.emb_clubs = nn.Embedding(voc_size_clubs, emb_size_clubs, device=device)\n",
    "\n",
    "    # Compute the total input size by adding the input size and the embedding sizes\n",
    "    total_input_size = input_size + emb_size_citizenship + emb_size_current_club_id + emb_size_position + emb_size_sub_position + emb_size_competitions + emb_size_clubs\n",
    "\n",
    "    # Define a linear layer that takes the total input size and outputs a single value\n",
    "    self.fc1 = nn.Linear(total_input_size, 1, device=device)\n",
    "\n",
    "  # Define the forward method that takes the numerical input and the categorical features to encode as arguments\n",
    "  def forward(self, x, to_encode):\n",
    "    # x: a tensor of shape (batch_size, input_size) containing the numerical features\n",
    "    # to_encode: a tensor of shape (batch_size, 4) containing the values of the categorical features\n",
    "\n",
    "    # Get the embeddings for each categorical feature using the corresponding embedding layer and indexing by the feature values\n",
    "    cit_emb = self.emb_cit(to_encode[:, 0])\n",
    "\n",
    "    club_emb = self.emb_club(to_encode[:, 1])\n",
    "\n",
    "    pos_emb = self.emb_pos(to_encode[:, 2])\n",
    "\n",
    "    sub_pos_emb = self.emb_sub_pos(to_encode[:, 3])\n",
    "\n",
    "    comp_emb = self.emb_comp(to_encode[:, 4])\n",
    "\n",
    "    clubs_emb = self.emb_clubs(to_encode[:, 5])\n",
    "\n",
    "    # Concatenate the numerical input and the embeddings along the second dimension\n",
    "    x = torch.cat((x, cit_emb, club_emb, pos_emb, sub_pos_emb, comp_emb, clubs_emb), dim=1)\n",
    "\n",
    "    # Apply the linear layer to get the output\n",
    "    output = self.fc1(x)\n",
    "\n",
    "    # Return the output\n",
    "    return output.flatten()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:02.485234600Z",
     "start_time": "2023-05-20T13:21:02.477185700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Define a multilayer perceptron model class that inherits from nn.Module\n",
    "class MLP(nn.Module):\n",
    "  # Define the constructor method that takes the input size and the vocabulary sizes of four categorical features as arguments\n",
    "  def __init__(self, input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, hidden_size1, hidden_size2, hidden_size3, dropout, device):\n",
    "    # Call the parent constructor\n",
    "    super(MLP, self).__init__()\n",
    "\n",
    "    # Compute the embedding sizes for each categorical feature using the fourth root of the vocabulary size\n",
    "    emb_size_citizenship = int(voc_size_citizenship ** (1/4))\n",
    "    emb_size_current_club_id = int(voc_size_current_club_id ** (1/4))\n",
    "    emb_size_position = int(voc_size_position ** (1/4))\n",
    "    emb_size_sub_position = int(voc_size_sub_position ** (1/4))\n",
    "    emb_size_competitions = int(voc_size_competitions ** (1/4))\n",
    "    emb_size_clubs = int(voc_size_clubs ** (1/4))\n",
    "\n",
    "    # Define embedding layers for each categorical feature using the computed embedding sizes\n",
    "    self.emb_cit = nn.Embedding(voc_size_citizenship, emb_size_citizenship, device=device)\n",
    "    self.emb_club = nn.Embedding(voc_size_current_club_id, emb_size_current_club_id, device=device)\n",
    "    self.emb_pos = nn.Embedding(voc_size_position, emb_size_position,  device=device)\n",
    "    self.emb_sub_pos = nn.Embedding(voc_size_sub_position, emb_size_sub_position, device=device)\n",
    "    self.emb_comp = nn.Embedding(voc_size_competitions, emb_size_competitions, device=device)\n",
    "    self.emb_clubs = nn.Embedding(voc_size_clubs, emb_size_clubs, device=device)\n",
    "\n",
    "    # Compute the total input size by adding the input size and the embedding sizes\n",
    "    total_input_size = input_size + emb_size_citizenship + emb_size_current_club_id + emb_size_position + emb_size_sub_position + emb_size_competitions + emb_size_clubs\n",
    "\n",
    "    # Define a linear layer that takes the total input size and outputs a single value\n",
    "    self.fc1 = nn.Linear(total_input_size, hidden_size1, device=device)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.batchnorm1 = nn.BatchNorm1d(hidden_size1, device=device)\n",
    "    self.fc2 = nn.Linear(hidden_size1, hidden_size2, device=device)\n",
    "    self.batchnorm2 = nn.BatchNorm1d(hidden_size2, device=device)\n",
    "    self.fc3 = nn.Linear(hidden_size2, hidden_size3, device=device)\n",
    "    self.batchnorm3 = nn.BatchNorm1d(hidden_size3, device=device)\n",
    "    self.fc4 = nn.Linear(hidden_size3, 1, device=device)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  # Define the forward method that takes the numerical input and the categorical features to encode as arguments\n",
    "  def forward(self, x, to_encode):\n",
    "    # x: a tensor of shape (batch_size, input_size) containing the numerical features\n",
    "    # to_encode: a tensor of shape (batch_size, 4) containing the values of the categorical features\n",
    "\n",
    "    # Get the embeddings for each categorical feature using the corresponding embedding layer and indexing by the feature values\n",
    "    cit_emb = self.emb_cit(to_encode[:, 0])\n",
    "\n",
    "    club_emb = self.emb_club(to_encode[:, 1])\n",
    "\n",
    "    pos_emb = self.emb_pos(to_encode[:, 2])\n",
    "\n",
    "    sub_pos_emb = self.emb_sub_pos(to_encode[:, 3])\n",
    "\n",
    "    comp_emb = self.emb_comp(to_encode[:, 4])\n",
    "\n",
    "    clubs_emb = self.emb_clubs(to_encode[:, 5])\n",
    "\n",
    "    # Concatenate the numerical input and the embeddings along the second dimension\n",
    "    x = torch.cat((x, cit_emb, club_emb, pos_emb, sub_pos_emb, comp_emb, clubs_emb), dim=1)\n",
    "\n",
    "    # Apply the linear layers, dropout and batchnorm to get the output\n",
    "    x = self.fc1(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = self.relu(x)\n",
    "\n",
    "    x = self.fc2(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.batchnorm2(x)\n",
    "    x = self.relu(x)\n",
    "\n",
    "    x = self.fc3(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.batchnorm3(x)\n",
    "    x = self.relu(x)\n",
    "\n",
    "    output = self.fc4(x)\n",
    "\n",
    "    # Return the output\n",
    "    return output.flatten()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:02.492877300Z",
     "start_time": "2023-05-20T13:21:02.485234600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Define a LSTM model class that inherits from nn.Module\n",
    "class LSTM(nn.Module):\n",
    "  # Define the constructor method that takes the input size and the vocabulary sizes of four categorical features as arguments\n",
    "  def __init__(self, input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, hidden_size1, num_layers, dropout, device):\n",
    "    # Call the parent constructor\n",
    "    super(LSTM, self).__init__()\n",
    "\n",
    "    # Compute the embedding sizes for each categorical feature using the fourth root of the vocabulary size\n",
    "    emb_size_citizenship = int(voc_size_citizenship ** (1/4))\n",
    "    emb_size_current_club_id = int(voc_size_current_club_id ** (1/4))\n",
    "    emb_size_position = int(voc_size_position ** (1/4))\n",
    "    emb_size_sub_position = int(voc_size_sub_position ** (1/4))\n",
    "    emb_size_competitions = int(voc_size_competitions ** (1/4))\n",
    "    emb_size_clubs = int(voc_size_clubs ** (1/4))\n",
    "\n",
    "    # Define embedding layers for each categorical feature using the computed embedding sizes\n",
    "    self.emb_cit = nn.Embedding(voc_size_citizenship, emb_size_citizenship, device=device)\n",
    "    self.emb_club = nn.Embedding(voc_size_current_club_id, emb_size_current_club_id, device=device)\n",
    "    self.emb_pos = nn.Embedding(voc_size_position, emb_size_position,  device=device)\n",
    "    self.emb_sub_pos = nn.Embedding(voc_size_sub_position, emb_size_sub_position, device=device)\n",
    "    self.emb_comp = nn.Embedding(voc_size_competitions, emb_size_competitions, device=device)\n",
    "    self.emb_clubs = nn.Embedding(voc_size_clubs, emb_size_clubs, device=device)\n",
    "\n",
    "    # Compute the total input size by adding the input size and the embedding sizes\n",
    "    total_input_size = input_size + emb_size_citizenship + emb_size_current_club_id + emb_size_position + emb_size_sub_position + emb_size_competitions + emb_size_clubs\n",
    "\n",
    "    self.lstm1 = nn.LSTM(total_input_size, hidden_size1, num_layers=num_layers, batch_first=True, dropout=dropout, device=device)\n",
    "\n",
    "    # Define a linear layer that takes the total input size and outputs a single value\n",
    "    self.fc1 = nn.Linear(hidden_size1, 1, device=device)\n",
    "\n",
    "\n",
    "\n",
    "  # Define the forward method that takes the numerical input and the categorical features to encode as arguments\n",
    "  def forward(self, x, to_encode):\n",
    "    # x: a tensor of shape (batch_size, input_size) containing the numerical features\n",
    "    # to_encode: a tensor of shape (batch_size, 4) containing the values of the categorical features\n",
    "\n",
    "    # Get the embeddings for each categorical feature using the corresponding embedding layer and indexing by the feature values\n",
    "    cit_emb = self.emb_cit(to_encode[:, 0])\n",
    "\n",
    "    club_emb = self.emb_club(to_encode[:, 1])\n",
    "\n",
    "    pos_emb = self.emb_pos(to_encode[:, 2])\n",
    "\n",
    "    sub_pos_emb = self.emb_sub_pos(to_encode[:, 3])\n",
    "\n",
    "    comp_emb = self.emb_comp(to_encode[:, 4])\n",
    "\n",
    "    clubs_emb = self.emb_clubs(to_encode[:, 5])\n",
    "\n",
    "    # Concatenate the numerical input and the embeddings along the second dimension\n",
    "    x = torch.cat((x, cit_emb, club_emb, pos_emb, sub_pos_emb, comp_emb, clubs_emb), dim=1)\n",
    "\n",
    "    # Add a dimension to the numerical input tensor corresponding to the sequence length\n",
    "    x = x[:, None, :]\n",
    "\n",
    "    # Apply the linear layer to get the output\n",
    "    out1, (h1, c1) = self.lstm1(x)\n",
    "    output = self.fc1(h1[0])\n",
    "\n",
    "    # Return the output\n",
    "    return output.flatten()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:02.564346Z",
     "start_time": "2023-05-20T13:21:02.500884Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#choose the model\n",
    "def build_model(model_type, input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, dropout):\n",
    "\n",
    "  if (model_type == \"linear\"):\n",
    "    return LinearRegression(input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, device)\n",
    "\n",
    "  elif (model_type == \"mlp\"):\n",
    "    hidden_size1 = 176\n",
    "    hidden_size2 = 64\n",
    "    hidden_size3 = 16\n",
    "    return MLP(input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, hidden_size1, hidden_size2, hidden_size3, dropout, device)\n",
    "\n",
    "  elif (model_type == \"lstm\"):\n",
    "    hidden_size1 = 32\n",
    "    num_layers = 2\n",
    "    return LSTM(input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, hidden_size1, num_layers, dropout, device)\n",
    "\n",
    "  elif (model_type == \"rf\"):\n",
    "    return RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "  else:\n",
    "    raise Exception(\"wrong model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:02.564346Z",
     "start_time": "2023-05-20T13:21:02.502764100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def build_optimizer(model, opt, lr, eps):\n",
    "  if (opt == \"adam\"):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, eps=eps)\n",
    "  elif (opt == \"sgd\"):\n",
    "    return torch.optim.SGD(model.parameters(), lr, momentum=0.9)\n",
    "  else:\n",
    "    raise Exception(\"wrong optimizer\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:02.564346Z",
     "start_time": "2023-05-20T13:21:02.509334300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def build_dataloaders(batch_size):\n",
    "  train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "  val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "  test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "  return train_loader, val_loader, test_loader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:02.564346Z",
     "start_time": "2023-05-20T13:21:02.516923700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def run(config=None):\n",
    "  # Initialize a new wandb run\n",
    "  with wandb.init(config=config):\n",
    "\n",
    "    # If called by wandb.agent, as below, this config will be set by Sweep Controller\n",
    "    config = wandb.config\n",
    "\n",
    "    r2score = 0\n",
    "    mse = 0\n",
    "    rmse = 0\n",
    "    mae = 0\n",
    "\n",
    "    metric = {\"r2score\": r2score, \"mse\": mse, \"rmse\": rmse, \"mae\": mae}\n",
    "\n",
    "    wandb.log(metric)\n",
    "\n",
    "    #Defining model, criterion, optimizer, scheduler and dataloaders\n",
    "    criterion = nn.MSELoss()\n",
    "    model_type = \"linear\"\n",
    "    model = build_model(model_type, input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, dropout=config.dropout)\n",
    "    wandb.log({\"model name\": model_type})\n",
    "\n",
    "    if (model_type != \"rf\"):\n",
    "\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.lr, config.eps)\n",
    "        #i want to log the name of the model\n",
    "\n",
    "        train_loader, val_loader, test_loader = build_dataloaders(config.batch_size)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs, eta_min=0.000001)\n",
    "\n",
    "        #Train and validation\n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=100)\n",
    "        train_losses, val_losses = train_val(model, criterion, optimizer, config.epochs, train_loader, val_loader, config.batch_size, config.lr)\n",
    "\n",
    "        #load the best model saved\n",
    "        model.load_state_dict(torch.load('models/{}_lr={}_bs={}_opt={}.pth'.format(model.__class__.__name__, config.lr, config.batch_size, type(optimizer).__name__)))\n",
    "        #Testing\n",
    "        all_targets, all_predictions = testing(model, test_loader)\n",
    "\n",
    "    else:\n",
    "        x_train = np.concatenate((train_to_encode, train_set), axis=1)\n",
    "        model.fit(x_train, y_train)\n",
    "        x_test = np.concatenate((test_to_encode, test_set), axis=1)\n",
    "        all_predictions = model.predict(x_test)\n",
    "        all_targets = y_test\n",
    "\n",
    "    #Saving test metrics\n",
    "    r2score = r2_score(all_targets, all_predictions)\n",
    "    mse = mean_squared_error(all_targets, all_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(all_targets, all_predictions)\n",
    "\n",
    "    metric = {\"r2score\": r2score, \"mse\": mse, \"rmse\": rmse, \"mae\": mae}\n",
    "\n",
    "    print(metric)\n",
    "\n",
    "    #Logging the metrics\n",
    "    wandb.log(metric)\n",
    "\n",
    "    #wandb.log({\"lr\": config.lr, \"batch_size\": config.batch_size, \"dropout\": config.dropout, \"optimizer\": config.optimizer, \"eps\": config.eps})\n",
    "\n",
    "    #terminate the run\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:02.564346Z",
     "start_time": "2023-05-20T13:21:02.521996800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def train_val(model, criterion, optimizer, epochs, train_loader, val_loader, batch_size, lr):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "    best_test_loss = np.inf\n",
    "    best_test_epoch = 0\n",
    "    i = 0\n",
    "    cont = 0\n",
    "\n",
    "    for it in tqdm(range(epochs)):\n",
    "      model.train()\n",
    "      t0 = datetime.now()\n",
    "      train_loss = []\n",
    "      total = dataset_train.length // batch_size\n",
    "      #with tqdm(total=total, position=0, leave=True) as pbar:\n",
    "      for inputs, to_encode, targets in train_loader:\n",
    "          #pbar.update()\n",
    "\n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          # Forward pass\n",
    "          outputs = model(inputs, to_encode)\n",
    "\n",
    "          loss = criterion(outputs, targets)\n",
    "\n",
    "          # Backward and optimize\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          train_loss.append(loss.item())\n",
    "\n",
    "      # Get train loss and test loss\n",
    "      train_loss = np.sum(train_loss)\n",
    "\n",
    "      model.eval()\n",
    "      test_loss = []\n",
    "      all_targets = []\n",
    "      all_predictions = []\n",
    "\n",
    "      for inputs, to_encode, targets in val_loader:\n",
    "\n",
    "        outputs = model(inputs, to_encode)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss.append(loss.item())\n",
    "        copy_prediction = outputs.clone()\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "        all_predictions.append(copy_prediction.detach().cpu().numpy())\n",
    "\n",
    "      test_loss = np.sum(test_loss)\n",
    "\n",
    "      #saving validation metrics\n",
    "      wandb.log({\n",
    "        'val_loss': test_loss,\n",
    "        'epochs': it+1\n",
    "      })\n",
    "\n",
    "      # Save losses\n",
    "      train_losses[it] = train_loss\n",
    "      test_losses[it] = test_loss\n",
    "\n",
    "      #We save the best model\n",
    "      if test_loss < best_test_loss:\n",
    "        torch.save(model.state_dict(), 'models/{}_lr={}_bs={}_opt={}.pth'.format(model.__class__.__name__, lr, batch_size, type(optimizer).__name__))\n",
    "        #artifact = wandb.Artifact('model', type='model')\n",
    "        #artifact.add_file('/content/drive/MyDrive/Output/FI/eliminami.pth')\n",
    "        #run.log_artifact(artifact)\n",
    "        #wandb.save(\"model.h5\")\n",
    "\n",
    "        best_test_loss = test_loss\n",
    "        best_test_epoch = it\n",
    "        #print('model saved')\n",
    "\n",
    "      dt = datetime.now() - t0\n",
    "      #print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "      #  Validation Loss: {test_loss:.4f}, Duration: {dt}, Best Val Epoch: {best_test_epoch}')\n",
    "\n",
    "    #torch.save(model, '/content/drive/MyDrive/Output/best_model_translob_FI')\n",
    "    return train_losses, test_losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:02.564346Z",
     "start_time": "2023-05-20T13:21:02.528649300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def testing(model, test_loader):\n",
    "  n_correct = 0.\n",
    "  n_total = 0.\n",
    "  all_targets = []\n",
    "  all_predictions = []\n",
    "\n",
    "  for inputs, to_encode, targets in test_loader:\n",
    "      # Forward pass\n",
    "      outputs = model(inputs, to_encode)\n",
    "      copy_prediction = outputs.clone()\n",
    "      all_targets.append(targets.cpu().numpy())\n",
    "      all_predictions.append(copy_prediction.detach().cpu().numpy())\n",
    "\n",
    "  all_targets = np.concatenate(all_targets)\n",
    "  all_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "  return all_targets, all_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:02.564346Z",
     "start_time": "2023-05-20T13:21:02.535736700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nclass config:\\n    def __init__(self, epochs, batch_size, lr, eps, weight_decay, optimizer):\\n        self.epochs = epochs\\n        self.batch_size = batch_size\\n        self.lr = lr\\n        self.eps = eps\\n        self.weight_decay = weight_decay\\n        self.optimizer = optimizer\\n        self.dropout = 0.3\\n\\nconfig = config(epochs=30, batch_size=128, lr=0.004783\\n, eps=1e-08, weight_decay=0.0, optimizer=\"sgd\")\\n\\nrun(config)\\n'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create class config\n",
    "'''\n",
    "class config:\n",
    "    def __init__(self, epochs, batch_size, lr, eps, weight_decay, optimizer):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.optimizer = optimizer\n",
    "        self.dropout = 0.3\n",
    "\n",
    "config = config(epochs=30, batch_size=128, lr=0.004783\n",
    ", eps=1e-08, weight_decay=0.0, optimizer=\"sgd\")\n",
    "\n",
    "run(config)\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:02.564346Z",
     "start_time": "2023-05-20T13:21:02.542222600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: fxson0nl\n",
      "Sweep URL: https://wandb.ai/leonardo-berti07/BDproject/sweeps/fxson0nl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: wfh7lu52 with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 128\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tdropout: 0.2\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tepochs: 50\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \teps: 1e-08\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlr: 0.007227206586239199\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \toptimizer: sgd\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.3"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\leona\\PycharmProjects\\BDC_project\\wandb\\run-20230520_152105-wfh7lu52</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/leonardo-berti07/BDproject/runs/wfh7lu52' target=\"_blank\">rare-sweep-1</a></strong> to <a href='https://wandb.ai/leonardo-berti07/BDproject' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/leonardo-berti07/BDproject/sweeps/fxson0nl' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDproject/sweeps/fxson0nl</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/leonardo-berti07/BDproject' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDproject</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View sweep at <a href='https://wandb.ai/leonardo-berti07/BDproject/sweeps/fxson0nl' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDproject/sweeps/fxson0nl</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/leonardo-berti07/BDproject/runs/wfh7lu52' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDproject/runs/wfh7lu52</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:31<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r2score': 0.9355402665044262, 'mse': 0.05766443, 'rmse': 0.2401342, 'mae': 0.11907993}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>mae</td><td>▁█</td></tr><tr><td>mse</td><td>▁█</td></tr><tr><td>r2score</td><td>▁█</td></tr><tr><td>rmse</td><td>▁█</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>50</td></tr><tr><td>mae</td><td>0.11908</td></tr><tr><td>model name</td><td>linear</td></tr><tr><td>mse</td><td>0.05766</td></tr><tr><td>r2score</td><td>0.93554</td></tr><tr><td>rmse</td><td>0.24013</td></tr><tr><td>val_loss</td><td>9.72457</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">rare-sweep-1</strong> at: <a href='https://wandb.ai/leonardo-berti07/BDproject/runs/wfh7lu52' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDproject/runs/wfh7lu52</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230520_152105-wfh7lu52\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: ztcbxp8j with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 32\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tdropout: 0.2\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tepochs: 50\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \teps: 1e-08\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlr: 0.00844277950653461\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \toptimizer: sgd\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.3"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\leona\\PycharmProjects\\BDC_project\\wandb\\run-20230520_152453-ztcbxp8j</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/leonardo-berti07/BDproject/runs/ztcbxp8j' target=\"_blank\">desert-sweep-2</a></strong> to <a href='https://wandb.ai/leonardo-berti07/BDproject' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/leonardo-berti07/BDproject/sweeps/fxson0nl' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDproject/sweeps/fxson0nl</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/leonardo-berti07/BDproject' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDproject</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View sweep at <a href='https://wandb.ai/leonardo-berti07/BDproject/sweeps/fxson0nl' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDproject/sweeps/fxson0nl</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/leonardo-berti07/BDproject/runs/ztcbxp8j' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDproject/runs/ztcbxp8j</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [09:50<00:00, 11.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r2score': 0.9275878342246927, 'mse': 0.06477852, 'rmse': 0.25451624, 'mae': 0.13917764}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>mae</td><td>▁█</td></tr><tr><td>mse</td><td>▁█</td></tr><tr><td>r2score</td><td>▁█</td></tr><tr><td>rmse</td><td>▁█</td></tr><tr><td>val_loss</td><td>▁▂▄▁▂█▁▁▁▁▁▁▁▁▂▁▃▂▁▂▂▁▂▂▁▁▁▃▂▁▂▂▂▁▅▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>50</td></tr><tr><td>mae</td><td>0.13918</td></tr><tr><td>model name</td><td>linear</td></tr><tr><td>mse</td><td>0.06478</td></tr><tr><td>r2score</td><td>0.92759</td></tr><tr><td>rmse</td><td>0.25452</td></tr><tr><td>val_loss</td><td>42.00909</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">desert-sweep-2</strong> at: <a href='https://wandb.ai/leonardo-berti07/BDproject/runs/ztcbxp8j' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDproject/runs/ztcbxp8j</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230520_152453-ztcbxp8j\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: 7k7fvn7s with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 32\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tdropout: 0.2\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tepochs: 50\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \teps: 1e-08\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlr: 0.006848764300486452\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \toptimizer: adam\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.3"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\leona\\PycharmProjects\\BDC_project\\wandb\\run-20230520_153506-7k7fvn7s</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/leonardo-berti07/BDproject/runs/7k7fvn7s' target=\"_blank\">sweet-sweep-3</a></strong> to <a href='https://wandb.ai/leonardo-berti07/BDproject' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/leonardo-berti07/BDproject/sweeps/fxson0nl' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDproject/sweeps/fxson0nl</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/leonardo-berti07/BDproject' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDproject</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View sweep at <a href='https://wandb.ai/leonardo-berti07/BDproject/sweeps/fxson0nl' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDproject/sweeps/fxson0nl</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/leonardo-berti07/BDproject/runs/7k7fvn7s' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDproject/runs/7k7fvn7s</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [2:30:33<10:39:20, 1826.68s/it]"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'goal': 'minimize',\n",
    "        'name': 'validation_loss'\n",
    "    },\n",
    "    'early_terminate' : {\n",
    "        'type': 'hyperband',\n",
    "        'min_iter': 3,\n",
    "        'eta': 2\n",
    "    },\n",
    "    'run_cap': 6\n",
    "  }\n",
    "\n",
    "parameters_dict = {\n",
    "    'epochs': {\n",
    "        'value': 50\n",
    "        },\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd']\n",
    "        },\n",
    "    'dropout': {\n",
    "          'values': [0.2]\n",
    "        },\n",
    "    'lr': {\n",
    "        'distribution': 'uniform',\n",
    "        'max': 0.01,\n",
    "        'min': 0.0001,\n",
    "        },\n",
    "    'batch_size': {\n",
    "        'values': [32, 128]\n",
    "        },\n",
    "    'eps': {\n",
    "        'value': 1e-08\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"BDproject\")\n",
    "wandb.agent(sweep_id, run, count=sweep_config[\"run_cap\"])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-20T13:21:02.548840800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
