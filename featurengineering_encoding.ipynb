{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Encoding\n",
    "**In this file, we will handle the cases in which the features values ​​are null or equal to 0. Next, some features need to be encoded to prepare the dataset for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BinaryType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.executor.memory', '15G'),\n",
       " ('spark.driver.host', 'MarcoPC.homenet.telecomitalia.it'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.memory', '50G'),\n",
       " ('spark.executor.cores', '10'),\n",
       " ('spark.app.name', 'PySparkProject'),\n",
       " ('spark.ui.port', '4050'),\n",
       " ('spark.driver.port', '51272'),\n",
       " ('spark.driver.maxResultSize', '40G'),\n",
       " ('spark.app.startTime', '1683974533368'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.id', 'local-1683974534216'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.submitTime', '1683974533202'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the session\n",
    "conf = SparkConf(). \\\n",
    "    set('spark.ui.port', \"4050\"). \\\n",
    "    set('spark.executor.memory', '15G'). \\\n",
    "    set('spark.driver.memory', '50G'). \\\n",
    "    set('spark.driver.maxResultSize', '40G'). \\\n",
    "    setAppName(\"PySparkProject\"). \\\n",
    "    set('spark.executor.cores', \"10\"). \\\n",
    "    setMaster(\"local[*]\")\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data.csv as pyspark dataframe\n",
    "df = spark.read.csv('data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the null values in the column last_valuation must be replaced with 0\n",
    "df = df.fillna({'last_valuation': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------+------------------+-------------------+----+---------------+------+-----------+----------+------------+--------------------+--------+-------+-----+--------------+---------+------------+--------------+-----------+------------+-------------+-------------+---------------+-----------------+--------------+---------------+---------------+-----------------+\n",
      "|player_id|             date_v|market_value|              name|         date_birth| age|current_club_id|height|citizenship|  position|sub_position|     competitions_id|clubs_id|assists|goals|minutes_played|red_cards|yellow_cards|last_valuation|appearances|games_won_pl|games_draw_pl|games_lost_pl|winning_rate_pl|games_played_club|games_won_club|games_draw_club|games_lost_club|winning_rate_club|\n",
      "+---------+-------------------+------------+------------------+-------------------+----+---------------+------+-----------+----------+------------+--------------------+--------+-------+-----+--------------+---------+------------+--------------+-----------+------------+-------------+-------------+---------------+-----------------+--------------+---------------+---------------+-----------------+\n",
      "|       26|2015-02-04 00:00:00|     3000000|Roman Weidenfeller|1980-08-06 00:00:00|34.0|             16|   190|    Germany|Goalkeeper|        null|        ['CL', 'L1']|    [16]|      0|    0|          1620|        0|           0|           0.0|         18|           7|            3|            8|            1.3|               50|            26|              7|             17|              1.7|\n",
      "|       26|2015-07-01 00:00:00|     2000000|Roman Weidenfeller|1980-08-06 00:00:00|34.0|             16|   190|    Germany|Goalkeeper|        null|        ['CL', 'L1']|    [16]|      0|    0|          2880|        0|           0|     3000000.0|         32|          14|            5|           13|            1.5|               49|            23|              8|             18|              1.6|\n",
      "|       26|2015-10-16 00:00:00|     1000000|Roman Weidenfeller|1980-08-06 00:00:00|35.0|             16|   190|    Germany|Goalkeeper|        null|['CL', 'ELQ', 'L1...|    [16]|      0|    0|          2610|        0|           0|     2000000.0|         29|          14|            5|           10|            1.6|               54|            29|             10|             15|              1.8|\n",
      "|       26|2016-02-15 00:00:00|     1000000|Roman Weidenfeller|1980-08-06 00:00:00|35.0|             16|   190|    Germany|Goalkeeper|        null|['CL', 'ELQ', 'L1...|    [16]|      0|    0|          1800|        0|           0|     1000000.0|         20|          11|            3|            6|            1.8|               54|            36|              7|             11|              2.1|\n",
      "|       26|2016-07-22 00:00:00|     1000000|Roman Weidenfeller|1980-08-06 00:00:00|35.0|             16|   190|    Germany|Goalkeeper|        null| ['ELQ', 'L1', 'EL']|    [16]|      0|    0|          1260|        0|           1|     1000000.0|         14|           9|            2|            3|            2.1|               56|            40|              8|              8|              2.3|\n",
      "+---------+-------------------+------------+------------------+-------------------+----+---------------+------+-----------+----------+------------+--------------------+--------+-------+-----+--------------+---------+------------+--------------+-----------+------------+-------------+-------------+---------------+-----------------+--------------+---------------+---------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the first 5 rows of the dataframe with sub_position null\n",
    "df.filter(df.sub_position.isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the null values in the column last_position must be replaced with the value in the column position\n",
    "df = df.withColumn(\"sub_position\", coalesce(col(\"sub_position\"), col(\"position\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop instances in which the column age or date_of_birth are null\n",
    "df = df.dropna(subset=('age', 'date_birth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dataframe to keep only the rows in which the column height is not 0\n",
    "filtered_df = df.filter(col(\"height\") != 0)\n",
    "\n",
    "# average height of filtered_df\n",
    "average_height = filtered_df.selectExpr(\"avg(height) as height_average\").first()[\"height_average\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the value 0 in the column height with the mean of the column \n",
    "df = df.withColumn(\"height\", when(col(\"height\") == 0, average_height).otherwise(col(\"height\")))\n",
    "# TODO fare in modo che dopo la virgola ci sia 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "# Assuming df is a Spark DataFrame\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"current_club_id\", outputCol=\"binarized_current_club_id\")\n",
    "binarized_df = binarizer.transform(df)\n",
    "print(binarized_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features that we have to manage are: \n",
    "- last valuation (13,45 % null --> from null to 0)\n",
    "- sub position (8,19 % null --> position)\n",
    "- age (0,04 % null --> delete examples)\n",
    "- date_birth (0,04 % null)\n",
    "- height (some values are 0 --> average height)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We transform the column competitions id and clubs id, that are array of strings in a single binary value using label binarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert clubs id and competitions id from list to string\n",
    "df = df.withColumn(\"competitions_id\", split(expr(\"substring(competitions_id, 2, length(competitions_id)-2)\"), \", \"))\n",
    "df = df.withColumn(\"clubs_id\", split(expr(\"substring(clubs_id, 2, length(clubs_id)-2)\"), \", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "c:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    }
   ],
   "source": [
    "# convert df in pandas dataframe\n",
    "df_pandas = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_14064\\1573311546.py:2: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  dummies = pd.get_dummies(df_pandas[\"competitions_id\"].apply(pd.Series).stack()).sum(level=0)\n"
     ]
    }
   ],
   "source": [
    "# apply pd.get_dummies to the column of arrays\n",
    "dummies = pd.get_dummies(df_pandas[\"competitions_id\"].apply(pd.Series).stack()).sum(level=0)\n",
    "# concatenate the dummy variables into a single string\n",
    "dummies[\"comp_string\"] = dummies.apply(lambda x: \"\".join(x.astype(str)), axis=1)\n",
    "# join the dummies dataframe with the original dataframe\n",
    "df_pandas = df_pandas.join(dummies[\"comp_string\"])\n",
    "#df_pandas = df_pandas.drop(\"competitions_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_14064\\1659577897.py:2: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  dummies = pd.get_dummies(df_pandas[\"clubs_id\"].apply(pd.Series).stack()).sum(level=0)\n"
     ]
    }
   ],
   "source": [
    "# apply pd.get_dummies to the column of arrays\n",
    "dummies = pd.get_dummies(df_pandas[\"clubs_id\"].apply(pd.Series).stack()).sum(level=0)\n",
    "# concatenate the dummy variables into a single string\n",
    "dummies[\"club_string\"] = dummies.apply(lambda x: \"\".join(x.astype(str)), axis=1)\n",
    "# join the dummies dataframe with the original dataframe\n",
    "df_pandas = df_pandas.join(dummies[\"club_string\"])\n",
    "#df_pandas = df_pandas.drop(\"club_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pandas = df_pandas.drop([\"competitions_id\", \"clubs_id\"], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "prima fare label binarization e poi nn.Encoding pytorch\n",
    "\n",
    "da encodare:\n",
    " - player_id\n",
    "  - date_v in timestemp\n",
    "  - current_club_id\n",
    "    - citizenship\n",
    "        - position\n",
    "- sub_position\n",
    "- comp_string\n",
    "- club_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "da droppare:\n",
    "    - name\n",
    "    - date_birth\n",
    "    - games_played_club\n",
    "    - games_won_club\n",
    "    - games_draw_club\n",
    "    - games_lost_club\n",
    "    - competitions_id\n",
    "    - clubs_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divided the column player_id for 1 million\n",
    "df_pandas[\"player_id\"] = df_pandas[\"player_id\"].apply(lambda x: x/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copia df_pandas in df_c\n",
    "df_c = df_pandas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = pd.concat([df_c, pd.get_dummies(df_c['current_club_id'])], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
