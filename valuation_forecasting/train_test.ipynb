{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Machine Learning Pipeline\n",
    "In this file there is the pipeline for the machine learning part of the project. The pipeline is composed by the following steps:\n",
    "1. Load the dataset\n",
    "2. Split the dataset into train, validation and test set\n",
    "3. Create the dataset class\n",
    "4. Create the model class\n",
    "5. Create the training loop\n",
    "6. Create the test loop\n",
    "\n",
    "For the hp search we use Weights and Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:43.104627500Z",
     "start_time": "2023-06-07T16:49:42.551147Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\leona/.netrc\n"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import pickle\n",
    "\n",
    "# Log in to your W&B account\n",
    "wandb.login(key='d29d51017f4231b5149d36ad242526b374c9c60a')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Controlling the setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:43.110175700Z",
     "start_time": "2023-06-07T16:49:43.104627500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'2.0.1'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:43.215059200Z",
     "start_time": "2023-06-07T16:49:43.112324500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "NVIDIA GeForce RTX 3090\n",
      "Wed Jun  7 18:49:43 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.61                 Driver Version: 531.61       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090       WDDM | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   46C    P8               25W / 350W|   1896MiB / 24576MiB |      4%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      5540      C   ...ona\\anaconda3\\envs\\torch\\python.exe    N/A      |\n",
      "|    0   N/A  N/A      5840    C+G   ...\\PyCharm 2023.1.1\\bin\\pycharm64.exe    N/A      |\n",
      "|    0   N/A  N/A      7580    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7876    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     10392    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     12144    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     14596    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14868    C+G   ...02.0_x86__zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     16672    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     17152    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     18832    C+G   ...0_x64__8wekyb3d8bbwe\\HxAccounts.exe    N/A      |\n",
      "|    0   N/A  N/A     20792    C+G   ....0_x64__8wekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     25028    C+G   ...on\\114.0.1823.37\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     25396    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     26216    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# The flag below controls whether to allow TF32 on matmul.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# The flag below controls whether to allow TF32 on cuDNN.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.cudnn.enabled)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:43.215059200Z",
     "start_time": "2023-06-07T16:49:43.192820300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "#to debug\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:43.463283100Z",
     "start_time": "2023-06-07T16:49:43.193360800Z"
    }
   },
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "df = pd.read_csv(\"dataset_normalized.csv\")\n",
    "total_rows = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:43.481923300Z",
     "start_time": "2023-06-07T16:49:43.480912700Z"
    }
   },
   "outputs": [],
   "source": [
    "#splitting the dataset into train, validation and test set (70%, 10%, 20%)\n",
    "train_end = int(total_rows*0.7)\n",
    "val_end = int(total_rows*0.8)\n",
    "\n",
    "labels = df[\"market_value\"].values\n",
    "df = df.drop(columns=[\"market_value\"])\n",
    "\n",
    "to_encode = df[['citizenship', 'current_club_id', 'position', 'sub_position', \"competitions_id\", \"clubs_id\"]]\n",
    "#to_encode = df[['citizenship', 'current_club_id', 'position', 'sub_position']]\n",
    "df = df.drop(['citizenship', 'current_club_id', 'position', 'sub_position', \"competitions_id\", \"clubs_id\"], axis=1)\n",
    "\n",
    "train_to_encode = to_encode.iloc[:train_end].values\n",
    "val_to_encode = to_encode.iloc[train_end:val_end].values\n",
    "test_to_encode = to_encode.iloc[val_end:].values\n",
    "\n",
    "train_set = df.iloc[:train_end].values\n",
    "val_set = df.iloc[train_end:val_end].values\n",
    "test_set = df.iloc[val_end:].values\n",
    "\n",
    "y_train = labels[:train_end]\n",
    "y_val = labels[train_end:val_end]\n",
    "y_test = labels[val_end:]\n",
    "\n",
    "train_set_len = train_set.shape[0]\n",
    "val_set_len = val_set.shape[0]\n",
    "test_set_len = test_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:43.529312100Z",
     "start_time": "2023-06-07T16:49:43.482999700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the len of train set is: 95114\n",
      "the len of validation set is: 13588\n",
      "the len of test set is: 27176\n"
     ]
    }
   ],
   "source": [
    "print(\"the len of train set is: {}\".format(train_set_len))\n",
    "print(\"the len of validation set is: {}\".format(val_set_len))\n",
    "print(\"the len of test set is: {}\".format(test_set_len))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create the pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:43.529312100Z",
     "start_time": "2023-06-07T16:49:43.496793200Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, x, to_encode, y, length):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "\n",
    "        self.length = length\n",
    "        self.to_encode = torch.tensor(to_encode, device=device, dtype=torch.int32)\n",
    "        self.y = torch.tensor(y, device=device, dtype=torch.float32)\n",
    "        self.x = torch.tensor(x, device=device, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.to_encode[i] ,self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.430970600Z",
     "start_time": "2023-06-07T16:49:43.503169100Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_val = Dataset(val_set, val_to_encode, y_val, val_set_len)\n",
    "dataset_test = Dataset(test_set, test_to_encode, y_test, test_set_len)\n",
    "dataset_train = Dataset(train_set, train_to_encode, y_train, train_set_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.441821300Z",
     "start_time": "2023-06-07T16:49:44.430970600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size of numerical features is: 18\n",
      "input size of categorical features is: 6\n",
      "voc size citizenship is: 162\n",
      "voc size current club id is: 391\n",
      "voc size position is: 4\n",
      "voc size sub position is: 16\n",
      "voc size competitions is: 2717\n",
      "voc size clubs is: 8054\n"
     ]
    }
   ],
   "source": [
    "#input parameters\n",
    "num_input_size = train_set.shape[1]\n",
    "cat_input_size = to_encode.shape[1]\n",
    "total_input_size = num_input_size + cat_input_size\n",
    "voc_size_citizenship = len(to_encode['citizenship'].unique())\n",
    "voc_size_current_club_id = len(to_encode['current_club_id'].unique())\n",
    "voc_size_position = len(to_encode['position'].unique())\n",
    "voc_size_sub_position = len(to_encode['sub_position'].unique())\n",
    "voc_size_competitions = len(to_encode['competitions_id'].unique())\n",
    "voc_size_clubs = len(to_encode['clubs_id'].unique())\n",
    "#print them all\n",
    "print(\"input size of numerical features is: {}\".format(num_input_size))\n",
    "print(\"input size of categorical features is: {}\".format(cat_input_size))\n",
    "print(\"voc size citizenship is: {}\".format(voc_size_citizenship))\n",
    "print(\"voc size current club id is: {}\".format(voc_size_current_club_id))\n",
    "print(\"voc size position is: {}\".format(voc_size_position))\n",
    "print(\"voc size sub position is: {}\".format(voc_size_sub_position))\n",
    "print(\"voc size competitions is: {}\".format(voc_size_competitions))\n",
    "print(\"voc size clubs is: {}\".format(voc_size_clubs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.453293500Z",
     "start_time": "2023-06-07T16:49:44.441821300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a linear regression model class that inherits from nn.Module\n",
    "class LinearRegression(nn.Module):\n",
    "  # Define the constructor method that takes the input size and the vocabulary sizes of four categorical features as arguments\n",
    "  def __init__(self, num_input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, device):\n",
    "    # Call the parent constructor\n",
    "    super(LinearRegression, self).__init__()\n",
    "\n",
    "    # Compute the embedding sizes for each categorical feature using the fourth root of the vocabulary size\n",
    "    emb_size_citizenship = int(voc_size_citizenship ** (1/4))\n",
    "    emb_size_current_club_id = int(voc_size_current_club_id ** (1/4))\n",
    "    emb_size_position = int(voc_size_position ** (1/4))\n",
    "    emb_size_sub_position = int(voc_size_sub_position ** (1/4))\n",
    "    emb_size_competitions = int(voc_size_competitions ** (1/4))\n",
    "    emb_size_clubs = int(voc_size_clubs ** (1/4))\n",
    "\n",
    "    # Define embedding layers for each categorical feature using the computed embedding sizes\n",
    "    self.emb_cit = nn.Embedding(voc_size_citizenship, emb_size_citizenship, device=device)\n",
    "    self.emb_club = nn.Embedding(voc_size_current_club_id, emb_size_current_club_id, device=device)\n",
    "    self.emb_pos = nn.Embedding(voc_size_position, emb_size_position,  device=device)\n",
    "    self.emb_sub_pos = nn.Embedding(voc_size_sub_position, emb_size_sub_position, device=device)\n",
    "    self.emb_comp = nn.Embedding(voc_size_competitions, emb_size_competitions, device=device)\n",
    "    self.emb_clubs = nn.Embedding(voc_size_clubs, emb_size_clubs, device=device)\n",
    "\n",
    "    # Compute the total input size by adding the input size and the embedding sizes\n",
    "    total_input_size = num_input_size + emb_size_citizenship + emb_size_current_club_id + emb_size_position + emb_size_sub_position + emb_size_competitions + emb_size_clubs\n",
    "\n",
    "    # Define a linear layer that takes the total input size and outputs a single value\n",
    "    self.fc1 = nn.Linear(total_input_size, 1, device=device)\n",
    "\n",
    "  # Define the forward method that takes the numerical input and the categorical features to encode as arguments\n",
    "  def forward(self, x, to_encode):\n",
    "    # x: a tensor of shape (batch_size, num_input_size) containing the numerical features\n",
    "    # to_encode: a tensor of shape (batch_size, 6) containing the values of the categorical features\n",
    "\n",
    "    # Get the embeddings for each categorical feature using the corresponding embedding layer and indexing by the feature values\n",
    "    cit_emb = self.emb_cit(to_encode[:, 0])\n",
    "\n",
    "    club_emb = self.emb_club(to_encode[:, 1])\n",
    "\n",
    "    pos_emb = self.emb_pos(to_encode[:, 2])\n",
    "\n",
    "    sub_pos_emb = self.emb_sub_pos(to_encode[:, 3])\n",
    "\n",
    "    comp_emb = self.emb_comp(to_encode[:, 4])\n",
    "\n",
    "    clubs_emb = self.emb_clubs(to_encode[:, 5])\n",
    "\n",
    "    # Concatenate the numerical input and the embeddings along the second dimension\n",
    "    x = torch.cat((x, cit_emb, club_emb, pos_emb, sub_pos_emb, comp_emb, clubs_emb), dim=1)\n",
    "\n",
    "    # Apply the linear layer to get the output\n",
    "    output = self.fc1(x)\n",
    "\n",
    "    # Return the output\n",
    "    return output.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.460480900Z",
     "start_time": "2023-06-07T16:49:44.453293500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a multilayer perceptron model class that inherits from nn.Module\n",
    "class MLP(nn.Module):\n",
    "  # Define the constructor method that takes the input size and the vocabulary sizes of four categorical features as arguments\n",
    "  def __init__(self, num_input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, hidden_size1, hidden_size2, hidden_size3, dropout, device):\n",
    "    # Call the parent constructor\n",
    "    super(MLP, self).__init__()\n",
    "\n",
    "    # Compute the embedding sizes for each categorical feature using the fourth root of the vocabulary size\n",
    "    emb_size_citizenship = int(voc_size_citizenship ** (1/4))\n",
    "    emb_size_current_club_id = int(voc_size_current_club_id ** (1/4))\n",
    "    emb_size_position = int(voc_size_position ** (1/4))\n",
    "    emb_size_sub_position = int(voc_size_sub_position ** (1/4))\n",
    "    emb_size_competitions = int(voc_size_competitions ** (1/4))\n",
    "    emb_size_clubs = int(voc_size_clubs ** (1/4))\n",
    "\n",
    "    # Define embedding layers for each categorical feature using the computed embedding sizes\n",
    "    self.emb_cit = nn.Embedding(voc_size_citizenship, emb_size_citizenship, device=device)\n",
    "    self.emb_club = nn.Embedding(voc_size_current_club_id, emb_size_current_club_id, device=device)\n",
    "    self.emb_pos = nn.Embedding(voc_size_position, emb_size_position,  device=device)\n",
    "    self.emb_sub_pos = nn.Embedding(voc_size_sub_position, emb_size_sub_position, device=device)\n",
    "    self.emb_comp = nn.Embedding(voc_size_competitions, emb_size_competitions, device=device)\n",
    "    self.emb_clubs = nn.Embedding(voc_size_clubs, emb_size_clubs, device=device)\n",
    "\n",
    "    # Compute the total input size by adding the input size and the embedding sizes\n",
    "    total_input_size = num_input_size + emb_size_citizenship + emb_size_current_club_id + emb_size_position + emb_size_sub_position + emb_size_competitions + emb_size_clubs\n",
    "\n",
    "    # Define a linear layer that takes the total input size and outputs a single value\n",
    "    self.fc1 = nn.Linear(total_input_size, hidden_size1, device=device)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.batchnorm1 = nn.BatchNorm1d(hidden_size1, device=device)\n",
    "    self.fc2 = nn.Linear(hidden_size1, hidden_size2, device=device)\n",
    "    self.batchnorm2 = nn.BatchNorm1d(hidden_size2, device=device)\n",
    "    self.fc3 = nn.Linear(hidden_size2, hidden_size3, device=device)\n",
    "    self.batchnorm3 = nn.BatchNorm1d(hidden_size3, device=device)\n",
    "    self.fc4 = nn.Linear(hidden_size3, 1, device=device)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  # Define the forward method that takes the numerical input and the categorical features to encode as arguments\n",
    "  def forward(self, x, to_encode):\n",
    "    # x: a tensor of shape (batch_size, input_size) containing the numerical features\n",
    "    # to_encode: a tensor of shape (batch_size, 6) containing the values of the categorical features\n",
    "\n",
    "    # Get the embeddings for each categorical feature using the corresponding embedding layer and indexing by the feature values\n",
    "    cit_emb = self.emb_cit(to_encode[:, 0])\n",
    "\n",
    "    club_emb = self.emb_club(to_encode[:, 1])\n",
    "\n",
    "    pos_emb = self.emb_pos(to_encode[:, 2])\n",
    "\n",
    "    sub_pos_emb = self.emb_sub_pos(to_encode[:, 3])\n",
    "\n",
    "    comp_emb = self.emb_comp(to_encode[:, 4])\n",
    "\n",
    "    clubs_emb = self.emb_clubs(to_encode[:, 5])\n",
    "\n",
    "    # Concatenate the numerical input and the embeddings along the second dimension\n",
    "    x = torch.cat((x, cit_emb, club_emb, pos_emb, sub_pos_emb, comp_emb, clubs_emb), dim=1)\n",
    "\n",
    "    # Apply the linear layers, dropout and batchnorm to get the output\n",
    "    x = self.fc1(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = self.relu(x)\n",
    "\n",
    "    x = self.fc2(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.batchnorm2(x)\n",
    "    x = self.relu(x)\n",
    "\n",
    "    x = self.fc3(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.batchnorm3(x)\n",
    "    x = self.relu(x)\n",
    "\n",
    "    output = self.fc4(x)\n",
    "\n",
    "    # Return the output\n",
    "    return output.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.546681Z",
     "start_time": "2023-06-07T16:49:44.460480900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a LSTM model class that inherits from nn.Module\n",
    "class LSTM(nn.Module):\n",
    "  # Define the constructor method that takes the input size and the vocabulary sizes of four categorical features as arguments\n",
    "  def __init__(self, num_input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, hidden_size1, num_layers, dropout, device):\n",
    "    # Call the parent constructor\n",
    "    super(LSTM, self).__init__()\n",
    "\n",
    "    # Compute the embedding sizes for each categorical feature using the fourth root of the vocabulary size\n",
    "    emb_size_citizenship = int(voc_size_citizenship ** (1/4))\n",
    "    emb_size_current_club_id = int(voc_size_current_club_id ** (1/4))\n",
    "    emb_size_position = int(voc_size_position ** (1/4))\n",
    "    emb_size_sub_position = int(voc_size_sub_position ** (1/4))\n",
    "    emb_size_competitions = int(voc_size_competitions ** (1/4))\n",
    "    emb_size_clubs = int(voc_size_clubs ** (1/4))\n",
    "\n",
    "    # Define embedding layers for each categorical feature using the computed embedding sizes\n",
    "    self.emb_cit = nn.Embedding(voc_size_citizenship, emb_size_citizenship, device=device)\n",
    "    self.emb_club = nn.Embedding(voc_size_current_club_id, emb_size_current_club_id, device=device)\n",
    "    self.emb_pos = nn.Embedding(voc_size_position, emb_size_position,  device=device)\n",
    "    self.emb_sub_pos = nn.Embedding(voc_size_sub_position, emb_size_sub_position, device=device)\n",
    "    self.emb_comp = nn.Embedding(voc_size_competitions, emb_size_competitions, device=device)\n",
    "    self.emb_clubs = nn.Embedding(voc_size_clubs, emb_size_clubs, device=device)\n",
    "\n",
    "    # Compute the total input size by adding the input size and the embedding sizes\n",
    "    total_input_size = num_input_size + emb_size_citizenship + emb_size_current_club_id + emb_size_position + emb_size_sub_position + emb_size_competitions + emb_size_clubs\n",
    "\n",
    "    self.lstm1 = nn.LSTM(total_input_size, hidden_size1, num_layers=num_layers, batch_first=True, dropout=dropout, device=device)\n",
    "\n",
    "    # Define a linear layer that takes the total input size and outputs a single value\n",
    "    self.fc1 = nn.Linear(hidden_size1, 1, device=device)\n",
    "\n",
    "\n",
    "\n",
    "  # Define the forward method that takes the numerical input and the categorical features to encode as arguments\n",
    "  def forward(self, x, to_encode):\n",
    "    # x: a tensor of shape (batch_size, num_input_size) containing the numerical features\n",
    "    # to_encode: a tensor of shape (batch_size, 6) containing the values of the categorical features\n",
    "\n",
    "    # Get the embeddings for each categorical feature using the corresponding embedding layer and indexing by the feature values\n",
    "    cit_emb = self.emb_cit(to_encode[:, 0])\n",
    "\n",
    "    club_emb = self.emb_club(to_encode[:, 1])\n",
    "\n",
    "    pos_emb = self.emb_pos(to_encode[:, 2])\n",
    "\n",
    "    sub_pos_emb = self.emb_sub_pos(to_encode[:, 3])\n",
    "\n",
    "    comp_emb = self.emb_comp(to_encode[:, 4])\n",
    "\n",
    "    clubs_emb = self.emb_clubs(to_encode[:, 5])\n",
    "\n",
    "    # Concatenate the numerical input and the embeddings along the second dimension\n",
    "    x = torch.cat((x, cit_emb, club_emb, pos_emb, sub_pos_emb, comp_emb, clubs_emb), dim=1)\n",
    "\n",
    "    # Add a dimension to the numerical input tensor corresponding to the sequence length\n",
    "    x = x[:, None, :]\n",
    "\n",
    "    # Apply the linear layer to get the output\n",
    "    out1, (h1, c1) = self.lstm1(x)\n",
    "    output = self.fc1(h1[0])\n",
    "\n",
    "    # Return the output\n",
    "    return output.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.546681Z",
     "start_time": "2023-06-07T16:49:44.467322400Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(model_type, num_input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, dropout):\n",
    "\n",
    "  if (model_type == \"linear\"):\n",
    "    return LinearRegression(num_input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, device)\n",
    "\n",
    "  elif (model_type == \"mlp\"):\n",
    "    hidden_size1 = 176\n",
    "    hidden_size2 = 64\n",
    "    hidden_size3 = 16\n",
    "    return MLP(num_input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, hidden_size1, hidden_size2, hidden_size3, dropout, device)\n",
    "\n",
    "  elif (model_type == \"lstm\"):\n",
    "    hidden_size1 = 32\n",
    "    num_layers = 2\n",
    "    return LSTM(num_input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, hidden_size1, num_layers, dropout, device)\n",
    "\n",
    "  else:\n",
    "    raise Exception(\"wrong model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.546681Z",
     "start_time": "2023-06-07T16:49:44.477345100Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_optimizer(model, opt, lr, eps):\n",
    "  if (opt == \"adam\"):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, eps=eps)\n",
    "  elif (opt == \"sgd\"):\n",
    "    return torch.optim.SGD(model.parameters(), lr, momentum=0.9)\n",
    "  else:\n",
    "    raise Exception(\"wrong optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.546681Z",
     "start_time": "2023-06-07T16:49:44.483790200Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_dataloaders(batch_size):\n",
    "  train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "  val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "  test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "  return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.546681Z",
     "start_time": "2023-06-07T16:49:44.491146800Z"
    }
   },
   "outputs": [],
   "source": [
    "def run(config=None):\n",
    "  # Initialize a new wandb run\n",
    "  with wandb.init(config=config):\n",
    "\n",
    "    # If called by wandb.agent, as below, this config will be set by Sweep Controller\n",
    "    config = wandb.config\n",
    "    wandb.log({\"model name\": model_type})\n",
    "\n",
    "    val_r2score = 0\n",
    "    val_mse = 0\n",
    "    val_rmse = 0\n",
    "    val_mae = 0\n",
    "\n",
    "    metric_val = {\"val_r2score\": val_r2score, \"val_mse\": val_mse, \"val_rmse\": val_rmse, \"val_mae\": val_mae}\n",
    "    wandb.log(metric_val)\n",
    "\n",
    "    test_r2score = 0\n",
    "    test_mse = 0\n",
    "    test_rmse = 0\n",
    "    test_mae = 0\n",
    "\n",
    "    metric_test = {\"test_r2score\": test_r2score, \"test_mse\": test_mse, \"test_rmse\": test_rmse, \"test_mae\": test_mae}\n",
    "    wandb.log(metric_test)\n",
    "\n",
    "    #Defining model, criterion, optimizer, scheduler and dataloaders\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if (model_type != \"rf\" and model_type != \"gbr\" and model_type != \"bagging\"):\n",
    "\n",
    "        model = build_model(model_type, num_input_size, voc_size_citizenship, voc_size_current_club_id, voc_size_position, voc_size_sub_position, voc_size_competitions, voc_size_clubs, dropout=config.dropout)\n",
    "\n",
    "\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.lr, config.eps)\n",
    "        #i want to log the name of the model\n",
    "\n",
    "        train_loader, val_loader, test_loader = build_dataloaders(config.batch_size)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs, eta_min=0.000001)\n",
    "\n",
    "        #Train and validation\n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=100)\n",
    "\n",
    "        all_targets, all_predictions = launch_model_pytorch(model, criterion, optimizer, scheduler, train_loader, val_loader, test_loader, config.epochs, config.batch_size, config.lr)\n",
    "\n",
    "    else:\n",
    "        if (model_type == \"gbr\"):\n",
    "            model = GradientBoostingRegressor(n_estimators=config.n_est, learning_rate=config.lr, max_features=config.max_features, max_depth=config.m_depth)\n",
    "\n",
    "        elif (model_type == \"bagging\"):\n",
    "            #setting max_features to 1.0 is equivalent to do bagging\n",
    "            model = RandomForestRegressor(n_estimators=config.n_est, max_features=1.0, max_depth=config.m_depth, n_jobs=4)\n",
    "\n",
    "        elif (model_type == \"rf\"):\n",
    "            model = RandomForestRegressor(n_estimators=config.n_est, max_features=config.max_features, max_depth=config.m_depth, n_jobs=4)\n",
    "        else:\n",
    "            raise Exception(\"wrong model type\")\n",
    "\n",
    "        all_predictions, all_targets = launch_model_sk(model, train_set, val_set, y_train, y_val)\n",
    "\n",
    "    #Saving test metrics\n",
    "    test_r2score = r2_score(all_targets, all_predictions)\n",
    "    test_mse = mean_squared_error(all_targets, all_predictions)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mae = mean_absolute_error(all_targets, all_predictions)\n",
    "\n",
    "    metric_test = {\"test_r2score\": test_r2score, \"test_mse\": test_mse, \"test_rmse\": test_rmse, \"test_mae\": test_mae}\n",
    "\n",
    "    print(metric_test)\n",
    "\n",
    "    #Logging the metrics\n",
    "    wandb.log(metric_test)\n",
    "\n",
    "    #terminate the run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.546681Z",
     "start_time": "2023-06-07T16:49:44.499096600Z"
    }
   },
   "outputs": [],
   "source": [
    "def launch_model_sk(model, train_set, val_set, y_train, y_val):\n",
    "    #training\n",
    "    x_train = np.concatenate((train_to_encode, train_set), axis=1)\n",
    "    model.fit(x_train, y_train)\n",
    "    #save the model\n",
    "    pickle.dump(model, open(\"models/best_{}\".format(model_type), 'wb'))\n",
    "\n",
    "    #validation\n",
    "    x_val = np.concatenate((val_to_encode, val_set), axis=1)\n",
    "    all_predictions_val = model.predict(x_val)\n",
    "    all_targets_val = y_val\n",
    "    val_r2score = r2_score(all_targets_val, all_predictions_val)\n",
    "    val_mse = mean_squared_error(all_targets_val, all_predictions_val)\n",
    "    val_rmse = np.sqrt(val_mse)\n",
    "    val_mae = mean_absolute_error(all_targets_val, all_predictions_val)\n",
    "    metric_val = {\"val_r2score\": val_r2score, \"val_mse\": val_mse, \"val_rmse\": val_rmse, \"val_mae\": val_mae}\n",
    "    wandb.log(metric_val)\n",
    "\n",
    "    #testing\n",
    "    x_test = np.concatenate((test_to_encode, test_set), axis=1)\n",
    "    all_predictions = model.predict(x_test)\n",
    "    all_targets = y_test\n",
    "\n",
    "    return all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.546681Z",
     "start_time": "2023-06-07T16:49:44.506304100Z"
    }
   },
   "outputs": [],
   "source": [
    "def launch_model_pytorch(model, criterion, optimizer, scheduler, train_loader, val_loader, test_loader, epochs, batch_size, lr):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    val_losses = np.zeros(epochs)\n",
    "    best_val_loss = np.inf\n",
    "    best_val_epoch = 0\n",
    "\n",
    "    for it in tqdm(range(epochs)):\n",
    "        t0 = datetime.now()\n",
    "        train_loss = train(model, criterion, optimizer, train_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "        val_loss = validate(model, criterion, val_loader)\n",
    "        wandb.log({\n",
    "            'val_loss': val_loss,\n",
    "            'epochs': it+1\n",
    "          })\n",
    "\n",
    "        # Save losses\n",
    "        train_losses[it] = train_loss\n",
    "        val_losses[it] = val_loss\n",
    "\n",
    "        #We save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save(model.state_dict(), 'models/{}_lr={}_bs={}_opt={}.pth'.format(model.__class__.__name__, lr, batch_size, type(optimizer).__name__))\n",
    "\n",
    "            best_val_loss = val_loss\n",
    "            best_val_epoch = it\n",
    "            #print('model saved')\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        #print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "        #  Validation Loss: {test_loss:.4f}, Duration: {dt}, Best Val Epoch: {best_test_epoch}')\n",
    "\n",
    "    #load the best model saved\n",
    "    model.load_state_dict(torch.load('models/{}_lr={}_bs={}_opt={}.pth'.format(model.__class__.__name__, lr, batch_size, type(optimizer).__name__)))\n",
    "    #Validate\n",
    "    final_val_loss = validate(model, criterion, val_loader)\n",
    "    wandb.log({\n",
    "        'final_val_loss': final_val_loss,\n",
    "      })\n",
    "\n",
    "    #Testing\n",
    "    all_targets, all_predictions = testing(model, test_loader)\n",
    "\n",
    "    return all_targets, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.546681Z",
     "start_time": "2023-06-07T16:49:44.514687600Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "\n",
    "    for inputs, to_encode, targets in train_loader:\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, to_encode)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "    return np.sum(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.546681Z",
     "start_time": "2023-06-07T16:49:44.520664600Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(model, criterion, val_loader):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for inputs, to_encode, targets in val_loader:\n",
    "\n",
    "        outputs = model(inputs, to_encode)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss.append(loss.item())\n",
    "        copy_prediction = outputs.clone()\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "        all_predictions.append(copy_prediction.detach().cpu().numpy())\n",
    "\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "    val_r2score = r2_score(all_targets, all_predictions)\n",
    "    val_mse = mean_squared_error(all_targets, all_predictions)\n",
    "    val_rmse = np.sqrt(val_mse)\n",
    "    val_mae = mean_absolute_error(all_targets, all_predictions)\n",
    "    metric_val = {\"val_r2score\": val_r2score, \"val_mse\": val_mse, \"val_rmse\": val_rmse, \"val_mae\": val_mae}\n",
    "    wandb.log(metric_val)\n",
    "\n",
    "    return np.sum(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.546681Z",
     "start_time": "2023-06-07T16:49:44.530835800Z"
    }
   },
   "outputs": [],
   "source": [
    "def testing(model, test_loader):\n",
    "  all_targets = []\n",
    "  all_predictions = []\n",
    "\n",
    "  for inputs, to_encode, targets in test_loader:\n",
    "      # Forward pass\n",
    "      outputs = model(inputs, to_encode)\n",
    "      copy_prediction = outputs.clone()\n",
    "      all_targets.append(targets.cpu().numpy())\n",
    "      all_predictions.append(copy_prediction.detach().cpu().numpy())\n",
    "\n",
    "  all_targets = np.concatenate(all_targets)\n",
    "  all_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "  return all_targets, all_predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Launching an Hyperparameter Search (a sweep of 10 runs) for every model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:49:44.546681Z",
     "start_time": "2023-06-07T16:49:44.534261300Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [\"rf\", \"gbr\", \"bagging\", \"mlp\", \"lstm\", \"linear\"]\n",
    "\n",
    "for model_type in models:\n",
    "    sweep_config = {\n",
    "        'method': 'random',\n",
    "        'metric': {\n",
    "            'goal': 'minimize',\n",
    "            'name': 'val_mse'\n",
    "        },\n",
    "        'early_terminate' : {\n",
    "            'type': 'hyperband',\n",
    "            'min_iter': 3,\n",
    "            'eta': 2\n",
    "        },\n",
    "        'run_cap': 10\n",
    "      }\n",
    "    if (model_type != \"rf\" and model_type != \"gbr\" and model_type != \"bagging\"):\n",
    "        parameters_dict = {\n",
    "            'epochs': {\n",
    "                'value': 50\n",
    "                },\n",
    "            'optimizer': {\n",
    "                'values': ['adam', 'sgd']\n",
    "                },\n",
    "            'dropout': {\n",
    "                  'values': [0.2]\n",
    "                },\n",
    "            'lr': {\n",
    "                'distribution': 'uniform',\n",
    "                'max': 0.01,\n",
    "                'min': 0.0001,\n",
    "                },\n",
    "            'batch_size': {\n",
    "                'values': [32, 128]\n",
    "                },\n",
    "            'eps': {\n",
    "                'value': 1e-08\n",
    "                }\n",
    "            }\n",
    "    elif (model_type == \"bagging\"):\n",
    "        parameters_dict = {\n",
    "            'm_depth': {\n",
    "                'distribution': 'int_uniform',\n",
    "                'max': total_input_size,\n",
    "                'min': 10,\n",
    "                },\n",
    "            'n_est':{\n",
    "                'distribution': 'int_uniform',\n",
    "                'max': 500,\n",
    "                'min': 50,\n",
    "                },\n",
    "            }\n",
    "    elif (model_type == 'rf'):\n",
    "        parameters_dict = {\n",
    "            'm_depth': {\n",
    "                'distribution': 'int_uniform',\n",
    "                'max': total_input_size,\n",
    "                'min': 10,\n",
    "                },\n",
    "            'n_est':{\n",
    "                'distribution': 'int_uniform',\n",
    "                'max': 500,\n",
    "                'min': 50,\n",
    "                },\n",
    "            'max_features':{\n",
    "                'distribution': 'int_uniform',\n",
    "                'max': total_input_size,\n",
    "                'min': 4,\n",
    "                },\n",
    "            }\n",
    "    elif (model_type == 'gbr'):\n",
    "        parameters_dict = {\n",
    "            'm_depth': {\n",
    "                'distribution': 'int_uniform',\n",
    "                'max': total_input_size,\n",
    "                'min': 10,\n",
    "                },\n",
    "            'n_est':{\n",
    "                'distribution': 'int_uniform',\n",
    "                'max': 500,\n",
    "                'min': 50,\n",
    "                },\n",
    "            'max_features':{\n",
    "                'distribution': 'int_uniform',\n",
    "                'max': total_input_size,\n",
    "                'min': 4,\n",
    "                },\n",
    "            'lr': {\n",
    "                'distribution': 'uniform',\n",
    "                'max': 1.0,\n",
    "                'min': 0.001,\n",
    "                },\n",
    "            }\n",
    "    parameters_dict['model_type'] = {\n",
    "        'value': model_type\n",
    "        }\n",
    "\n",
    "    sweep_config['parameters'] = parameters_dict\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"BDproject2\")\n",
    "    wandb.agent(sweep_id, run, count=sweep_config[\"run_cap\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T17:15:56.935901Z",
     "start_time": "2023-06-07T17:15:21.733382600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.15.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.3"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\leona\\PycharmProjects\\BDC_project\\valuation_forecasting\\wandb\\run-20230607_191521-ivn71qq4</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/leonardo-berti07/BDC_project-valuation_forecasting/runs/ivn71qq4' target=\"_blank\">crisp-tree-3</a></strong> to <a href='https://wandb.ai/leonardo-berti07/BDC_project-valuation_forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/leonardo-berti07/BDC_project-valuation_forecasting' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDC_project-valuation_forecasting</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/leonardo-berti07/BDC_project-valuation_forecasting/runs/ivn71qq4' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDC_project-valuation_forecasting/runs/ivn71qq4</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_r2score': 0.9561701986008412, 'test_mse': 0.04715347911198005, 'test_rmse': 0.21714851855810588, 'test_mae': 0.08774438604767303}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "870623f9a946406fab258ac3d7e56ddf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_mae</td><td>▁█</td></tr><tr><td>test_mse</td><td>▁█</td></tr><tr><td>test_r2score</td><td>▁█</td></tr><tr><td>test_rmse</td><td>▁█</td></tr><tr><td>val_mae</td><td>▁█</td></tr><tr><td>val_mse</td><td>▁█</td></tr><tr><td>val_r2score</td><td>▁█</td></tr><tr><td>val_rmse</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>model name</td><td>rf</td></tr><tr><td>test_mae</td><td>0.08774</td></tr><tr><td>test_mse</td><td>0.04715</td></tr><tr><td>test_r2score</td><td>0.95617</td></tr><tr><td>test_rmse</td><td>0.21715</td></tr><tr><td>val_mae</td><td>0.08465</td></tr><tr><td>val_mse</td><td>0.04687</td></tr><tr><td>val_r2score</td><td>0.95599</td></tr><tr><td>val_rmse</td><td>0.21649</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">crisp-tree-3</strong> at: <a href='https://wandb.ai/leonardo-berti07/BDC_project-valuation_forecasting/runs/ivn71qq4' target=\"_blank\">https://wandb.ai/leonardo-berti07/BDC_project-valuation_forecasting/runs/ivn71qq4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230607_191521-ivn71qq4\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create class config used to test and debug\n",
    "\n",
    "model_type = \"rf\"\n",
    "class config:\n",
    "    def __init__(self, epochs, batch_size, eps, weight_decay, optimizer, model_type):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.optimizer = optimizer\n",
    "        self.dropout = 0.3\n",
    "        self.model_type = model_type\n",
    "        self.n_est = 143\n",
    "        self.m_depth = 20\n",
    "        self.lr = 0.274\n",
    "        self.max_features = 20\n",
    "\n",
    "config = config(epochs=30, batch_size=128, eps=1e-08, weight_decay=0.0, optimizer=\"sgd\", model_type=model_type)\n",
    "\n",
    "run(config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
